{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copia di Copia di Copia di NeuralNetworks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Elisshaze/into2ML/blob/main/NeuralNetworks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duf_GclnPTDH"
      },
      "source": [
        "!pip install split-folders"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NBiolG1PPIw"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import splitfolders\n",
        "import torch.utils.data as data\n",
        "import torch.utils.tensorboard as tb\n",
        "from torchvision import datasets, transforms as T\n",
        "import pandas as pd\n",
        "import torch.utils.tensorboard as tb\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler \n",
        "# This enables running tensorboard in the notebook\n",
        "%load_ext tensorboard\n",
        "import torch.nn as nn\n",
        "BATCH_SIZE = 128"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDMgadc4yaCQ"
      },
      "source": [
        "cuda = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYi6oIkpPYNm"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mo-gNAMXPmKz"
      },
      "source": [
        "! tar -zxvf /content/drive/MyDrive/into2ML/dataset.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOXh0Fh3P19m"
      },
      "source": [
        "!mkdir -p dataset/test/unknown\n",
        "!mv dataset/test/*.png dataset/test/unknown/"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkXToa9P0C9e"
      },
      "source": [
        "num_classes = 10"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Om_gg7E1ToMH"
      },
      "source": [
        "transform_train = T.Compose([T.Resize(256), \n",
        "                       T.CenterCrop(224), \n",
        "                       T.ToTensor(),\n",
        "])\n",
        "transform_test = T.Compose([\n",
        "                            T.Resize(224),\n",
        "                            T.ToTensor(),\n",
        "])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQN6O47TaQAR",
        "outputId": "6e98a6e6-7a80-4a0e-b8ad-cb579bb5f803"
      },
      "source": [
        "splitfolders.ratio(\"./dataset/train\", output=\"./dataset/train_val\", seed=1337, ratio=(.8, .2), group_prefix=None) # default values"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying files: 12415 files [00:48, 257.65 files/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAL_wh9eVhmB"
      },
      "source": [
        "trDataSet = torchvision.datasets.ImageFolder('./dataset/train_val/train/', transform = transform_train)\n",
        "vlDataSet = torchvision.datasets.ImageFolder('./dataset/train_val/val/', transform = transform_test)\n",
        "tsDataSet = torchvision.datasets.ImageFolder('./dataset/test/', transform = transform_test )"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3zgGECXbCdg"
      },
      "source": [
        "Normalization code taken from https://pytorch.org/vision/stable/models.html, as I am not using the pretrained model in this case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1tSdO6HUOrG",
        "outputId": "438ac79a-49b5-44aa-dc77-5f17bc66061f"
      },
      "source": [
        "means = []\n",
        "stds = []\n",
        "for img, _ in (trDataSet):\n",
        "    means.append(torch.mean(img))\n",
        "    stds.append(torch.std(img))\n",
        "\n",
        "mean = torch.mean(torch.tensor(means))\n",
        "std = torch.mean(torch.tensor(stds))\n",
        "print (mean, std)\n",
        "\n",
        "normalize_tr = T.Normalize(mean=mean, std=std)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.1713) tensor(0.1216)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJf9eyh-dghT",
        "outputId": "7936d1e5-39a6-4f3e-eaf0-d3761eead543"
      },
      "source": [
        "for img, _ in (tsDataSet):\n",
        "    means.append(torch.mean(img))\n",
        "    stds.append(torch.std(img))\n",
        "\n",
        "mean = torch.mean(torch.tensor(means))\n",
        "std = torch.mean(torch.tensor(stds))\n",
        "print (mean, std)\n",
        "\n",
        "normalize_ts = T.Normalize(mean=mean, std=std)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.1685) tensor(0.1169)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHO0LXZrdR2M"
      },
      "source": [
        "transform_train = T.Compose([\n",
        "                       T.ToTensor(),\n",
        "                       normalize_tr,\n",
        "])\n",
        "transform_test = T.Compose([\n",
        "                            T.ToTensor(),\n",
        "                            normalize_ts,\n",
        "])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fTwvZe1jLfN"
      },
      "source": [
        "trDataSet = torchvision.datasets.ImageFolder('./dataset/train_val/train/', transform = transform_train)\n",
        "vlDataSet = torchvision.datasets.ImageFolder('./dataset/train_val/val/', transform = transform_test)\n",
        "tsDataSet = torchvision.datasets.ImageFolder('./dataset/test/', transform = transform_test )"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHBmXQCwRv54"
      },
      "source": [
        "trDataLoader = torch.utils.data.DataLoader(trDataSet, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True, drop_last=True)\n",
        "vlDataLoader = torch.utils.data.DataLoader(vlDataSet, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True, drop_last=True)\n",
        "tsDataLoader = torch.utils.data.DataLoader(tsDataSet, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True, drop_last=True)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ShBC2sRkZCp"
      },
      "source": [
        "model = torchvision.models.resnet50(pretrained = False, progress = True)\n",
        "model.fc = nn.Sequential(nn.Linear(2048, 512),\n",
        "                                 nn.ReLU(),\n",
        "                                 nn.Dropout(0.2),\n",
        "                                 nn.Linear(512, 10),\n",
        "                                 nn.LogSoftmax(dim=1))\n",
        "model = model.to(cuda)\n",
        "print (model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeBvkM56p_Y_"
      },
      "source": [
        "#Performance evaluation\n",
        "class ClassificationMetrics:\n",
        "\n",
        "  # Constructor takes the number of classes\n",
        "  def __init__(self, num_classes=10):\n",
        "    self.num_classes = num_classes\n",
        "    # Initialize a confusion matrix\n",
        "    self.C = torch.zeros(num_classes, num_classes) \n",
        "    self.C = self.C.to(cuda)\n",
        "\n",
        "  # Update the confusion matrix with the new scores\n",
        "  def add(self, yp, yt):\n",
        "    # yp: 1D tensor with predictions\n",
        "    # yt: 1D tensor with ground-truth targets\n",
        "\n",
        "    with torch.no_grad(): # We require no computation graph\n",
        "      yt = yt.to(cuda)\n",
        "      yp = yp.to(cuda)\n",
        "      self.C+=(yt*self.C.shape[1]+yp).bincount(minlength=self.C.numel()).view(self.C.shape).float()\n",
        "\n",
        "  def clear(self):\n",
        "    # We set the confusion matrix to zero\n",
        "    self.C.zero_()\n",
        "\n",
        "  # Computes the global accuracy\n",
        "  def acc(self):\n",
        "    return self.C.diag().sum().item()/self.C.sum()\n",
        "\n",
        "  # Computes the class-averaged accuracy\n",
        "  def mAcc(self):\n",
        "    return (self.C.diag()/self.C.sum(-1)).mean().item()\n",
        "\n",
        "  # Computers the class-averaged Intersection over Union\n",
        "  def mIoU(self):\n",
        "    return (self.C.diag()/(self.C.sum(0)+self.C.sum(1)-self.C.diag())).mean().item()\n",
        "\n",
        "  # Returns the confusion matrix\n",
        "  def confusion_matrix(self):\n",
        "    return self.C"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXloY88xbE51"
      },
      "source": [
        "def train_one_epoch(model, loss_func, metric_tracker, dataloader, optimizer, epoch, tblog=None):\n",
        "  # Function to perform one training epoch\n",
        "  #\n",
        "  # model: PyTorch model to train\n",
        "  # loss_func: training loss to be used\n",
        "  # metric_tracker: classification metric tracker\n",
        "  # dataloader: dataloader with the training data\n",
        "  # optimizer:PyTorch optimizer to use\n",
        "  # epoch: current epoch number\n",
        "  # tblog: tensorboard summary writer\n",
        "\n",
        "  \n",
        "  # We set the model in training mode. Some layers, typically involving a \n",
        "  # stochastic component, behave differently if run in evaluation vs training \n",
        "  # mode (e.g., dropout, batch normalization, etc.). For this reason we need\n",
        "  # to specify the modality.\n",
        "  model.train()\n",
        "\n",
        "  # Clear the metric tracker\n",
        "  metric_tracker.clear()\n",
        "\n",
        "  # We iterate over each mini-batch by tracking the iteration number\n",
        "  for i,(X,yt) in enumerate(dataloader):\n",
        "    X = X.to(cuda)\n",
        "    yt = yt.to(cuda)\n",
        "    # Set gradients to zero\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    # We assume that the model outputs a 2D tensor, where the second dimension\n",
        "    # indexes class labels\n",
        "    Y = model(X)\n",
        "\n",
        "    # Compute the loss \n",
        "    loss = loss_func(Y, yt)\n",
        "\n",
        "    # The index of the largest output along the second dimension gives the predicted\n",
        "    # class label\n",
        "    y = Y.argmax(-1)\n",
        "\n",
        "    # Track evaluation metrics\n",
        "    metric_tracker.add(y, yt)\n",
        "\n",
        "    # If a tensorboard summary writer is available we track the loss value\n",
        "    # and the metrics of each iteration\n",
        "    if tblog:\n",
        "      tblog.add_scalar('train/loss', loss.item(), epoch*len(dataloader)+i)\n",
        "\n",
        "    # Perform the backward pass to compute gradients\n",
        "    loss.backward()\n",
        "    val_loss = loss.item()\n",
        "\n",
        "    # Update the parameters\n",
        "    optimizer.step()\n",
        "    return val_loss"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbbNu2mpqHpO"
      },
      "source": [
        "def validate(model, metric_tracker, dataloader):\n",
        "  # Function to perform a validation of the current model\n",
        "  #\n",
        "  # model: PyTorch model to train\n",
        "  # metric_tracker: classification metric tracker\n",
        "  # dataloader: dataloader with the validatoin data\n",
        "\n",
        "  # We set the model in evaluation mode. Some layers, typically involving a \n",
        "  # stochastic compponent, behave differently if run in evaluation vs training \n",
        "  # mode (e.g., dropout, batch normalization, etc.). For this reason we need\n",
        "  # to specify the modality.\n",
        "  model.eval()\n",
        "\n",
        "  # Clear the metric tracker\n",
        "  metric_tracker.clear()\n",
        "\n",
        "  # This informs PyTorch that no gradient computation is needed and, therefore,\n",
        "  # it should not track the computation graph\n",
        "  with torch.no_grad(): \n",
        "    # We iterate over each mini-batch by tracking the iteration number\n",
        "    for i,(X,yt) in enumerate(dataloader):\n",
        "        X = X.to(cuda)\n",
        "        yt = yt.to(cuda)\n",
        "        model.to(cuda)\n",
        "      # Forward pass\n",
        "      # We assume that the model outputs a 2D tensor, where the second dimension\n",
        "      # indexes class labels\n",
        "        Y = model(X)\n",
        "\n",
        "      # The index of the largest output along the second dimension gives the predicted\n",
        "      # class label\n",
        "        y = Y.argmax(-1)\n",
        "\n",
        "      # Track evaluation metrics\n",
        "        metric_tracker.add(y,yt)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hvs49_uqKBj"
      },
      "source": [
        "def train(model, trDataLoader, vlDataLoader, optimizer, lr_scheduler, num_epochs, tblog=None):\n",
        "  # Training function\n",
        "  #\n",
        "  # model: PyTorch model to train (we assume it exposes a num_classes function)\n",
        "  # trDataLoader: dataloader for training data\n",
        "  # vlDataLoader: dataloader for validation data\n",
        "  # optimizer: the PyTorch optimizer to use\n",
        "  # lr_scheduler: the PyTorch scheduler for the learning rate\n",
        "  # num_epochs: number of epochs to train the model\n",
        "  # tblog: tensorboard logger\n",
        "\n",
        "  # We create the loss function to be used for training\n",
        "  loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "  # We create the performance metric tracker\n",
        "  metric_tracker = ClassificationMetrics(num_classes)\n",
        "\n",
        "  for epoch in range(1,num_epochs+1):\n",
        "\n",
        "    print(\"-- EPOCH {}/{} -------------------------\\n\".format(epoch, num_epochs))\n",
        "\n",
        "    # Train for one epoch\n",
        "    val_loss= train_one_epoch(model, loss_func, metric_tracker, trDataLoader, optimizer, epoch, tblog)\n",
        "\n",
        "    # Print metrics accumulated over the epoch\n",
        "    print(\"\\tTRAIN | acc: {:.4f} | mAcc: {:.4f} | mIoU: {:.4f}\".format(\n",
        "        metric_tracker.acc(), metric_tracker.mAcc(), metric_tracker.mIoU()\n",
        "    ))\n",
        "    \n",
        "    # Track the metrics in the tensorboard log\n",
        "    if tblog:\n",
        "      tblog.add_scalar('train/acc', metric_tracker.acc(), epoch)\n",
        "      tblog.add_scalar('train/mAcc', metric_tracker.mAcc(), epoch)\n",
        "      tblog.add_scalar('train/mIoU', metric_tracker.mIoU(), epoch)\n",
        "      print(\"VAL_LOSS: \", val_loss)\n",
        "      #tblog.add_scalar('train/lr', scheduler._last_lr, epoch)\n",
        "\n",
        "    # Evaluate the current model\n",
        "    validate(model, metric_tracker, vlDataLoader)\n",
        "\n",
        "    # Print metrics over the validation set\n",
        "    print(\"\\tEVAL  | acc: {:.4f} | mAcc: {:.4f} | mIoU: {:.4f}\\n\".format(\n",
        "        metric_tracker.acc(), \n",
        "        metric_tracker.mAcc(), metric_tracker.mIoU()\n",
        "    ))\n",
        "\n",
        "    # Track the metrics in the tensorboard log\n",
        "    if tblog:\n",
        "      tblog.add_scalar('val/acc', metric_tracker.acc(), epoch)\n",
        "      tblog.add_scalar('val/mAcc', metric_tracker.mAcc(), epoch)\n",
        "      tblog.add_scalar('val/mIoU', metric_tracker.mIoU(), epoch)\n",
        "\n",
        "    # Update the learning rate according to the scheduler\n",
        "\n",
        "    lr_scheduler.step(val_loss)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwWmD05hqM9d"
      },
      "source": [
        "def test(model, dataloader):\n",
        "  # Function to validate the model and print the final score\n",
        "  #\n",
        "  # model: PyTorch model to test (we assume it exposes a num_classes function)\n",
        "  # dataloader: dataloader for test data\n",
        "\n",
        "  # We create the performance metric tracker\n",
        "  metric_tracker = ClassificationMetrics(num_classes)\n",
        "\n",
        "  # We run the validation code con the test data and track the performance\n",
        "  validate(model,metric_tracker, dataloader)\n",
        "\n",
        "  # Print metrics over the test set\n",
        "  print(\"TEST  | acc: {:.4f} | mAcc: {:.4f} | mIoU: {:.4f}\\n\".format(\n",
        "      metric_tracker.acc(), \n",
        "      metric_tracker.mAcc(), metric_tracker.mIoU()\n",
        "  ))\n",
        "\n",
        "  return metric_tracker\n",
        "\n",
        "# Here we apply the test function to the model we trained before\n",
        "#test(model, tsDataLoader);"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeKb47cxqTQx"
      },
      "source": [
        "def run_experiment(name, model):\n",
        "\n",
        "  # We create a simple Stochastic Gradient Descent (SGD) optimizer, with momentum \n",
        "  # and weight-decay (i.e. l2 regularization)\n",
        "  optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=0.00001)\n",
        "\n",
        "  # We add a learning rate scheduler that reduces the learning rate by a factor\n",
        "  # 0.1 after 10 and 15 epochs, respectively\n",
        "\n",
        "  #optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum = 0.9)\n",
        "  #lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, [10,15], gamma=0.5)\n",
        "\n",
        "  \n",
        "  lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=1, verbose=True)\n",
        "  #for epoch in range(10):\n",
        "  # We train our model\n",
        "  tblog=tb.SummaryWriter(\"exps/{}\".format(name))\n",
        "  train(model, trDataLoader, vlDataLoader, optimizer, lr_scheduler, num_epochs=40, tblog=tblog)\n",
        "  #val_loss = validate(...)\n",
        "  # Note that step should be called after validate()\n",
        "\n",
        "  # We create the tensorboard logger\n",
        " \n",
        "\n",
        "  \n",
        "\n",
        "# Run an experiment with an instance of our SimpleMLP with 256 and 128 units in the first and \n",
        "# second hidden layers, respectively.\n",
        "#model = SimpleMLP(784, 256, 128, 10)\n",
        "#run_experiment('MLP-256-128-Sigmoid', model)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zaD_RalqOwe"
      },
      "source": [
        "%tensorboard --logdir exps/resnNet50_1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "2cnfUH7QqWgf",
        "outputId": "6b2e70bb-18f2-4497-8347-7d88ac6e58b3"
      },
      "source": [
        "results={}\n",
        "name='resnNet50_1'\n",
        "run_experiment(name, model)\n",
        "!mkdir -p 'dataset/save/resnNet50_1'\n",
        "torch.save(model, 'dataset/save/resnet50_1')\n",
        "results[name]= test(model, tsDataLoader)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-- EPOCH 1/40 -------------------------\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-c9d578defbad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'resnNet50_1'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mkdir -p 'dataset/save/resnNet50_1'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dataset/save/resnet50_1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-8e4fec01fc7c>\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(name, model)\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;31m# We train our model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0mtblog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSummaryWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"exps/{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m   \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvlDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtblog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtblog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m   \u001b[0;31m#val_loss = validate(...)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0;31m# Note that step should be called after validate()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-4273e121a11e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, trDataLoader, vlDataLoader, optimizer, lr_scheduler, num_epochs, tblog)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# Train for one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mval_loss\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_tracker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtblog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# Print metrics accumulated over the epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-77a99b2f1a19>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, loss_func, metric_tracker, dataloader, optimizer, epoch, tblog)\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0;31m# We iterate over each mini-batch by tracking the iteration number\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0myt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# Set gradients to zero\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 14.76 GiB total capacity; 13.46 GiB already allocated; 37.75 MiB free; 13.69 GiB reserved in total by PyTorch)"
          ]
        }
      ]
    }
  ]
}